theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = disney_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Disney Return") +
theme_minimal()
apple_model <- lm(apple_return ~ nasdaq_return, data = stocks)
summary(apple_model)
# a = 0.0627
# b = 1.0912
disney_model <- lm(disney - nasdaq_return, data = stocks)
disney_model <- lm(disney_return - nasdaq_return, data = stocks)
View(stocks)
mean(stocks$disney_return, na.rm = TRUE)
summary(stocks$disney_return)
disney_model <- lm(disney_return ~ nasdaq_return, data = stocks)
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
stat_smooth(mapping = aes(x = nasdaq_return, y = apple_return), data = stocks,
method = "lm", geom = "smooth")
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
geom_point() +
stat_smooth(mapping = aes(x = nasdaq_return, y = apple_return), data = stocks,
method = "lm", geom = "smooth")
# Examples related to OpenIntro Chapter 5
# Sec5.1 Point Estimates and sampling variability
###---- Toy simulation: choosing small values to see what we are doing
pop_size = 20
p=0.8 # true population proportion
n=4 # sample size
# 1. Create poll responses
possible_responses = c(rep("S", 0.8*pop_size),rep("N", 0.2*pop_size))
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
print(possible_responses)
# 2. Sample 4 responses without replacement
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
#3. Proportion of responses with "Support" (S)
sum(sample_responses == "S") / 4
#Repeat the poll 5 times
computed_phats = rep(0,5)
for (k in 1:5) {
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
phat = sum(sample_responses == "S") / 4
print(paste('phat=',phat))
computed_phats[k]=phat
}
print(computed_phats)
# -----------------------------------------------------------------
### --- Simulating polls from population = 250 million American adults
pop_size = 250000000
p=0.88 # true population proportion
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
n=1000/4 # sample size
n_polls = 10000 # Number of polls
computed_phats = rep(0,n_polls)
for (k in 1:n_polls) {
sample_responses = sample(possible_responses, size = n)
phat = sum(sample_responses == "S") / n
computed_phats[k]=phat
}
hist(computed_phats,breaks=40)
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
# In Lab 3, you compare theoretical and empirical probabilities
# Recall: sum(dairyqueen$cal_fat > 600)/length(dairyqueen$cal_fat)
#
# Examples related to OpenIntro Chapter 5
# Sec5.1 Point Estimates and sampling variability
###---- Toy simulation: choosing small values to see what we are doing
pop_size = 20
p=0.8 # true population proportion
n=4 # sample size
# 1. Create poll responses
possible_responses = c(rep("S", 0.8*pop_size),rep("N", 0.2*pop_size))
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
print(possible_responses)
# 2. Sample 4 responses without replacement
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
#3. Proportion of responses with "Support" (S)
sum(sample_responses == "S") / 4
#Repeat the poll 5 times
computed_phats = rep(0,5)
for (k in 1:5) {
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
phat = sum(sample_responses == "S") / 4
print(paste('phat=',phat))
computed_phats[k]=phat
}
print(computed_phats)
# -----------------------------------------------------------------
### --- Simulating polls from population = 250 million American adults
pop_size = 250000000
p=0.88 # true population proportion
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
n=1000/4 # sample size
n_polls = 10000 # Number of polls
computed_phats = rep(0,n_polls)
for (k in 1:n_polls) {
sample_responses = sample(possible_responses, size = n)
phat = sum(sample_responses == "S") / n
computed_phats[k]=phat
}
hist(computed_phats,breaks=40)
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
# In Lab 3, you compare theoretical and empirical probabilities
# Recall: sum(dairyqueen$cal_fat > 600)/length(dairyqueen$cal_fat)
#
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
pop_size = 250000000
p=0.88 # true population proportion
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
n=1000/4 # sample size
n_polls = 10000 # Number of polls
computed_phats = rep(0,n_polls)
for (k in 1:n_polls) {
sample_responses = sample(possible_responses, size = n)
phat = sum(sample_responses == "S") / n
computed_phats[k]=phat
}
hist(computed_phats,breaks=40)
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
# In Lab 3, you compare theoretical and empirical probabilities
# Recall: sum(dairyqueen$cal_fat > 600)/length(dairyqueen$cal_fat)
#
dpois(4, 5)
1 - dpois(4, 5)
ppois(4, 5)
1 - ppois(4, 5)
1 - ppois(3, 5)
1 - ppois(3, 5)
1 - pposoi(9, 20)
1 - ppoois(9, 20)
1 - ppois(9, 20)
ppois(9, 20)
dpois(9, 20)
1 - dpois(9, 20)
dpois(9, 20)
ppois(9, 20)
ppois(10, 20)
dgeom(1500, 0.002)
1- dgeom(1500, 0.002)
dbinom(1500, 0.002)
dbinom(8,1500, 0.002)
head(iris)
1/9 * 1/36
.41 + .37 _ .16
.41 + .37 + .16
(0 * .41) + (1 * .37) + (2 * .16) + (3 * 3.05) + (4 * .01)
(0 * .41) + (1 * .37) + (2 * .16) + (3 * .05) + (4 * .01)
.41 + .37
.78 _ .16
.78 + .16
(0 * .41) + (1 * .37) + (4 * .16) + (9 * .05) + (16 * .01)
.88 * .88
1.62 - .7744
sqrt(0.8456)
0.05 + 0.02 + 0.05 + 0.10 + 0.03 + 0.10
0.1 + 0.03 + 0.10 + 0.05 + 0.02 + 0.05
data(iris)
mean(iris$Petal.Width)
sd(iris$Petal.Width)
fivenum(Petal.Length)
fivenum(iris$Petal.Length)
five_num <- fivenum(iris$Petal.Length)
help(five_num)
help(fivenum)
quantile(iris$Petal.Length, 0.25)
1 - quantile(iris$Petal.Length, 0.60)
quantile(iris$Petal.Length, 0.60)
fivenum(iris.Petal.Length)
fivenum(iris$Petal.Length)
quantile(iris$Petal.Length, 0.60)
dpois(500, 0.09)
ppois(500, 0.09)
pgeom(500, 0.09)
pbinom(0.91, 500)
pbinom(0.91, 500, 0.09)
pbinom(500, 5000, 0.09)
stdev <- 4.78
mu <- 72.6
qnorm(80, mu, stdev)
pnorm(, mu, stdev)
1 -pnorm(80, mu, stdev)
1 -pnorm(80, 72.6, 4.78)
pnorm(80, 72.6, 4.78) - pnorm(60, 72.6, 4.78)
qnorm(0.05, 72.6, 4.78)
1 -qnorm(0.05, 72.6, 4.78)
qnorm(0.05, 72.6, 4.78)
qnorm(0.95, 72.6, 4.78)
pnbinom(7, 6, .55)
pnbinom(7, 6, .55)
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.975)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(lowerBound, upperBound)
c(phat-MOE,phat+MOE)
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.9)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(lowerBound, upperBound)
c(phat-MOE,phat+MOE)
zcrit = qnorm(0.99)
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.99)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(lowerBound, upperBound)
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
zcrit = qnorm(0.975)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
zcrit = qnorm(0.975)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
zcrit = qnorm(0.975)
MOE = zcrit*SE
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.975)
MOE = zcrit*SE
c(phat-MOE,phat+MOE)
qnorm(0.01475)
# random sample of 1,042
1042 * .82
phat <- (1042*.82) / 1042
phat <- (1042*.82) / 1042
se <- sqrt((phat*(1-phat))/1042)
moe <- 1.96 * phat
lower <- phat - moe
upper <- phat + moe
# poll from 2014
# 82% of New Yorkers favor a manatory quarantine
# for anyone who came into contact with an EBola patient
# random sample of 1,042
phat <- (1042*.82) / 1042
se <- sqrt((phat*(1-phat))/1042)
moe <- 1.96 * phat
lower <- phat - moe
upper <- phat + moe
# poll from 2014
# 82% of New Yorkers favor a manatory quarantine
# for anyone who came into contact with an EBola patient
# random sample of 1,042
phat <- (1042*.82) / 1042
se <- sqrt((phat*(1-phat))/1042)
moe <- 1.96 * se
lower <- phat - moe
upper <- phat + moe
lower
upper
pnorm(0.58)
1 - qnorm(0.58)
# area to the right
1 - pnorm(2.53)
# area to the right
1 - pnorm(0.58, 0.5, 0.0316)
# area to the right
1 - pnorm(2.53)
# area to the right
1 - pnorm(0.58, 0.5, 0.0316)
# area to the right
1 - pnorm(2.53)
spending$state_code <- substr(spending$coc_number, 1, 2)
# Lab 7, March 27th, 2025
library(dplyr)
library(ggplot2)
library(janitor)
library(readxl)
setwd("/Users/davidsosa/Code/school/regression-analysis/labs/lab7")
life_exp <- read_excel("data/life_expectancy_insurance_example_data.xlsx", sheet="Life Expectancy")
########
# homeless data
spending <- read_excel("data/homelessness_data.xlsx", "Federal Aid")
spending$state_code <- substr(spending$coc_number, 1, 2)
state_spending <- spending |>
group_by(state_code) |>
summarize(funding = sum(amount))
homelessness <- read_excel("data/homelessness_data.xlsx", "Homelessness")
homelessness <- read_excel("data/homelessness_data.xlsx", "Homelessness")
census <- read_excel("data/homelessness_data.xlsx", "Census Data")
100000*(homelessness$total_homless_pop/census$total_population)
data$avg_homeless <- 100000*(homelessness$total_homless_pop/census$total_population)
merged_data <- merge(homelessness, census, by="state_name")
merged_data <- merge(homelessness, census, by.x="state_name", by.y="STATE")
View(merged_data)
merged_data$avg_homeless_per_100k <- (merged_data$total_homless_pop/merged_data$total_population) * 100000
ggplot(merged_data, aes(x = poverty_rate, y = avg_homeless_per_100k)) +
geom_point() +
theme_minimal()
merged_data$poverty_rate <- (merged_data$under.5 + merged_data$pov_0.5to0.99) / merged_data$total
ggplot(merged_data, aes(x = poverty_rate, y = avg_homeless_per_100k)) +
geom_point() +
theme_minimal()
ggplot(merged_data, aes(x = poverty_rate, y = avg_homeless_per_100k)) +
geom_point() +
labs(
x = "Povery Rate",
y = "Homelessness Per 100k",
title = "Homelessness vs Poverty Rate"
) +
theme_minimal()
summary(lm(poverty_rate ~ avg_homeless_per_100k, data = merged_data))
summary(lm(poverty_rate ~ avg_homeless_per_100k, data = merged_data))
summary(lm(poverty_rate ~ avg_homeless_per_100k, data = merged_data))
summary(lm(avg_homeless_per_100k ~ poverty_rate, data = merged_data))
summary(merged_data$poverty_rate)
summary(lm(poverty_rate ~ avg_homeless_per_100k, data = merged_data))
summary(lm(avg_homeless_per_100k ~ poverty_rate, data = merged_data))
summary(lm(avg_homeless_per_100k ~ poverty_rate, data = merged_data))
merged_data$poverty_rate <- (merged_data$under.5 + merged_data$pov_0.5to0.99) / merged_data$total
summary(merged_data$poverty_rate)
summary(lm(avg_homeless_per_100k ~ poverty_rate, data = merged_data))
setwd("/Users/davidsosa/Code/school/regression-analysis/labs/regression")
setwd("/Users/davidsosa/Code/school/regression-analysis/labs/regression")
cps <- readxl::read_excel("Multiple Regression Example Data.xlsx", sheet = "CPS")
cps <- readxl::read_excel("Multiple Regression Example Data.xlsx", sheet = "CPS")
cps <- readxl::read_excel("Multiple Regression Example Data.xlsx", sheet = "CPS")
homelessness <- read_excel("data/homelessness_data.xlsx", "Homelessness")clear
cpi <- readxl::read_excel("Multiple Regression Example Data.xlsx", sheet = "CPI")
cps <- merge(cps, pwl, by.x = c("year", "state"), by.y = c("year", "fips"))
setwd("/Users/davidsosa/Code/school/regression-analysis/labs/regression")
cps <- readxl::read_excel("Multiple Regression Example Data.xlsx", sheet = "CPS")
pwl <- readxl::read_excel("Multiple Regression Example Data.xlsx", sheet = "PWL")
cpi <- readxl::read_excel("Multiple Regression Example Data.xlsx", sheet = "CPI")
cps <- merge(cps, pwl, by.x = c("year", "state"), by.y = c("year", "fips"))
cps <- merge(cps, pwl, by.x = c("year", "statefip"), by.y = c("year", "fips"))
cps <- merge(cps, cpi, by.x = "year", by.y = "Year")
rm(cpi, pwl)
library(dplyr)
# turn nominal income into real income
cps <- cps |>
mutate(real_income = incwage*CPI/0.652)
View(cps)
# check I made it correct
graph_data <- cps |>
group_by(year) |>
summarize(real_income=median(real_inc),
nominal_income = median(incwage))
# check I made it correct
graph_data <- cps |>
group_by(year) |>
summarize(real_income=median(real_income),
nominal_income = median(incwage))
ggplot(graph_data)
ggplot(graph_data) +
geom_line(aes(x = year, y = nominal_income))
ggplot(graph_data) +
geom_line(aes(x = year, y = nominal_income)) +
geom_line(aes(x = year, y = real_income), color = "blue") +
theme_classic()
cps_bc <- cps |>
filter(occ50ly > 499 & occ50ly < 700) |>
mutate(construction = ifelse(ind50ly == 246, 1, 0),
race_new = ifelse(race == 100, "1 - White",
ifelse(rae == 200, "2 - Black",
ifelse(race == 300, "3 - Native America",
"4. - Other")))
)
cps_bc <- cps |>
filter(occ50ly > 499 & occ50ly < 700) |>
mutate(construction = ifelse(ind50ly == 246, 1, 0),
race_new = ifelse(race == 100, "1 - White",
ifelse(race == 200, "2 - Black",
ifelse(race == 300, "3 - Native America",
"4. - Other")))
)
View(cps)
View(cps_bc)
sum(cps_bc$construction/length(cps_bc$construction))
sum(cps_bc$construction/length(cps_bc$construction))
pwd_reg <- lm(real_income ~ pw, cps_bc)
summary(pwd_reg)
# multiple
pw_reg2 <- lm(real_income ~ pw + construction + pw*construction + age +
as.factor(sex) + race_new + state + as.factor(year), data =cps_bc)
summary(pw_reg2)
# Lab 7, March 27th, 2025
library(dplyr)
library(ggplot2)
library(janitor)
library(readxl)
setwd("/Users/davidsosa/Code/school/regression-analysis/labs/lab7")
life_exp <- read_excel("data/life_expectancy_insurance_example_data.xlsx", sheet="Life Expectancy")
head(life_exp)
life_exp <- life_exp |>
filter(Year == 2010) |>
clean_names() |>
select(state, county, fips, female_le, male_le)
income <- read_excel("data/life_expectancy_insurance_example_data.xlsx", sheet="Income")
income$fips <- income$STATEA*1000 + income$COUNTYA
income <- income |>
clean_names() |>
select(fips, median_hh_income)
life_exp <- merge(life_exp, income, by = "fips")
# insurance part
insurance <- read_excel("data/life_expectancy_insurance_example_data.xlsx",sheet ="Insurance")
insurance$FIPS = 1000*insurance$STATEA + insurance$COUNTYA
uninsured <- insurance |>
clean_names() |>
group_by(fips) |>
summarize(uninsured_rate = (male_under65_uninsured + female_under65_uninsured)/(male_under65_total + female_under65_total))
life_exp <- merge(life_exp, uninsured, by = "fips")
region <- read_excel("data/life_expectancy_insurance_example_data.xlsx",sheet ="Region")
life_exp <- merge(life_exp, region, by.x = "state", by.y = "STATE")
rm(income, insurance, region, uninsured)
cor(life_exp$uninsured_rate, life_exp$female_le)
ggplot(data = life_exp) +
geom_point(aes(x = uninsured_rate, y = female_le)) +
xlab("Uninsured Rate") +
ylab("Female Life Expectancy") +
theme_classic()
summary(lm(female_le ~ uninsured_rate, data = life_exp))
summary(life_exp$uninsured_rate)
summary(lm(female_le ~ uninsured_rate + median_hh_income, data = life_exp))
summary(lm(female_le ~ uninsured_rate + median_hh_income + Region, data = life_exp))
########
# homeless data
spending <- read_excel("data/homelessness_data.xlsx", "Federal Aid")
spending$state_code <- substr(spending$coc_number, 1, 2)
state_spending <- spending |>
group_by(state_code) |>
summarize(funding = sum(amount))
homelessness <- read_excel("data/homelessness_data.xlsx", "Homelessness")
census <- read_excel("data/homelessness_data.xlsx", "Census Data")
merged_data <- merge(homelessness, census, by.x="state_name", by.y="STATE")
spending <- read_excel("data/homelessness_data.xlsx", "Federal Aid")
spending$state_code <- substr(spending$coc_number, 1, 2)
state_spending <- spending |>
group_by(state_code) |>
summarize(funding = sum(amount))
homelessness <- read_excel("data/homelessness_data.xlsx", "Homelessness")
census <- read_excel("data/homelessness_data.xlsx", "Census Data")
merged_data <- merge(homelessness, census, by.x="state_name", by.y="STATE")
census <- read_excel("data/homelessness_data.xlsx", "Census Data")
merged_data <- merge(homelessness, census, by.x="state_name", by.y="STATE")
merged_data <- merge(homelessness, census, by = "state_name")
merged_data$avg_homeless_per_100k <- (merged_data$total_homeless_pop/merged_data$total_population) * 100000
homelessness <- read_excel("data/homelessness_data.xlsx", "Homelessness")
census <- read_excel("data/homelessness_data.xlsx", "Census Data")
merged_data <- merge(homelessness, census, by = "state_name")
View(merged_data)
merged_data$avg_homeless_per_100k <- (merged_data$total_homeless_pop/merged_data$total_population) * 100000
ggplot(merged_data, aes(x = poverty_rate, y = avg_homeless_per_100k)) +
geom_point() +
labs(
x = "Povery Rate",
y = "Homelessness Per 100k",
title = "Homelessness vs Poverty Rate"
) +
theme_minimal()
merged_data$avg_homeless_per_100k <- (merged_data$total_homeless_pop/merged_data$total_population) * 100000
merged_data$poverty_rate <- (merged_data$under.5 + merged_data$pov_0.5to0.99) / merged_data$total
ggplot(merged_data, aes(x = poverty_rate, y = avg_homeless_per_100k)) +
geom_point() +
labs(
x = "Povery Rate",
y = "Homelessness Per 100k",
title = "Homelessness vs Poverty Rate"
) +
theme_minimal()
summary(lm(avg_homeless_per_100k ~ poverty_rate, data = merged_data))
summary(lm(avg_homeless_per_100k ~ poverty_rate, data = merged_data))
merged_data$poverty_rate <- merged_data$poverty_rate * 100
summary(lm(avg_homeless_per_100k ~ poverty_rate, data = merged_data))
