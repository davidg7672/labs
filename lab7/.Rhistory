values_from = open) |>
rename(nasdaq = `^IXIC`)
stocks <- stocks |>
select(symbol, date, open) |>
pivot_wider(names_from = symbol,
values_from = open) |>
rename(nasdaq = `^IXIC`)
stocks <- stocks |>
select(symbol, date, open) |>
pivot_wider(names_from = symbol,
values_from = open) |>
rename(nasdaq =`^IXIC`)
stocks <- tq_get(c("AAPL", "DIS", "^IXIC"),
get = "stock.prices",
from = "2019-01-01",
to = "2024-01-01")
stocks <- stocks |>
select(symbol, date, open) |>
pivot_wider(names_from = symbol,
values_from = open) |>
rename(nasdaq =`^IXIC`)
stocks <- stocks |>
mutate(apple_return = (AAPL/lag(AAPL) -1)*100,
disney_return = (DIS/lag(DIS)-1)*100,
nasdaq_return = (nasdq/lag(nasdaq)-1)*100)
stocks <- stocks |>
mutate(apple_return = (AAPL/lag(AAPL)-1)*100,
disney_return = (DIS/lag(DIS)-1)*100,
nasdaq_return = (nasdaq/lag(nasdaq)-1)*100)
summary(stocks$disney_return)
summary(stocks$apple_return)
summary(stocks$nasdaq)
mean(stocks$disney_return)
mean(stocks$apple_return)
mean(stocks$nasdaq)
summary(stocks$disney_return)
summary(stocks$apple_return)
summary(stocks$nasdaq)
summary(stocks$disney_return)
summary(stocks$apple_return)
summary(stocks$nasdaq)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyquant)
stocks <- tq_get(c("AAPL", "DIS", "^IXIC"),
get = "stock.prices",
from = "2019-01-01",
to = "2024-01-01")
stocks <- stocks |>
select(symbol, date, open) |>
pivot_wider(names_from = symbol,
values_from = open) |>
rename(nasdaq =`^IXIC`)
stocks <- stocks |>
mutate(apple_return = (AAPL/lag(AAPL)-1)*100,
disney_return = (DIS/lag(DIS)-1)*100,
nasdaq_return = (nasdaq/lag(nasdaq)-1)*100)
summary(stocks$disney_return)
summary(stocks$apple_return)
summary(stocks$nasdaq)
summary(stocks$nasdaq_return)
mean(stocks$apple_return)
mean(stocks$disney_return)
mean(stocks$apple_return)
mean(stocks$nasdaq_return)
mean(stocks$disney_return, na.rm = TRUE)
mean(stocks$apple_return, na.rm = TRUE)
mean(stocks$nasdaq_return, na.rm = TRUE)
stocks$date[which.min(stocks$nasdaq_return)]
(tail(stocks$nasdaq, 1)/head(stocks$nasdaq, 1)-1)* 100
(tail(stocks$DIS, 1)/head(stocks$DIS, 1)-1)* 100
(tail(stocks$AAPL, 1)/head(stocks$AAPL, 1)-1)* 100
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
geom_point() +
labs(x = "NASQAQ Return",
y = "Apple Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Apple Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = apple_return, color = stocks)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Apple Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Apple Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = disney_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Disney Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = disney_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Disney Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Apple Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = disney_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Disney Return") +
theme_minimal()
apple_model <- lm(apple_return ~ nasdaq_return, data = stocks)
summary(apple_model)
View(apple_model)
disney_model <- lm(disney_return - nasdaq_return, data = stocks)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidyquant)
stocks <- tq_get(c("AAPL", "DIS", "^IXIC"),
get = "stock.prices",
from = "2019-01-01",
to = "2024-01-01")
stocks <- stocks |>
select(symbol, date, open) |>
pivot_wider(names_from = symbol,
values_from = open) |>
rename(nasdaq =`^IXIC`)
stocks <- stocks |>
mutate(apple_return = (AAPL/lag(AAPL)-1)*100,
disney_return = (DIS/lag(DIS)-1)*100,
nasdaq_return = (nasdaq/lag(nasdaq)-1)*100)
summary(stocks$disney_return)
summary(stocks$apple_return)
summary(stocks$nasdaq_return)
mean(stocks$disney_return, na.rm = TRUE)
mean(stocks$apple_return, na.rm = TRUE)
mean(stocks$nasdaq_return, na.rm = TRUE)
stocks$date[which.min(stocks$nasdaq_return)]
(tail(stocks$nasdaq, 1)/head(stocks$nasdaq, 1)-1)* 100
(tail(stocks$DIS, 1)/head(stocks$DIS, 1)-1)* 100
(tail(stocks$AAPL, 1)/head(stocks$AAPL, 1)-1)* 100
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Apple Return") +
theme_minimal()
ggplot(stocks, aes(x = nasdaq_return, y = disney_return)) +
geom_point() +
xlab("NASDAQ Return") +
ylab("Disney Return") +
theme_minimal()
apple_model <- lm(apple_return ~ nasdaq_return, data = stocks)
summary(apple_model)
# a = 0.0627
# b = 1.0912
disney_model <- lm(disney - nasdaq_return, data = stocks)
disney_model <- lm(disney_return - nasdaq_return, data = stocks)
View(stocks)
mean(stocks$disney_return, na.rm = TRUE)
summary(stocks$disney_return)
disney_model <- lm(disney_return ~ nasdaq_return, data = stocks)
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
stat_smooth(mapping = aes(x = nasdaq_return, y = apple_return), data = stocks,
method = "lm", geom = "smooth")
ggplot(stocks, aes(x = nasdaq_return, y = apple_return)) +
geom_point() +
stat_smooth(mapping = aes(x = nasdaq_return, y = apple_return), data = stocks,
method = "lm", geom = "smooth")
# Examples related to OpenIntro Chapter 5
# Sec5.1 Point Estimates and sampling variability
###---- Toy simulation: choosing small values to see what we are doing
pop_size = 20
p=0.8 # true population proportion
n=4 # sample size
# 1. Create poll responses
possible_responses = c(rep("S", 0.8*pop_size),rep("N", 0.2*pop_size))
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
print(possible_responses)
# 2. Sample 4 responses without replacement
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
#3. Proportion of responses with "Support" (S)
sum(sample_responses == "S") / 4
#Repeat the poll 5 times
computed_phats = rep(0,5)
for (k in 1:5) {
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
phat = sum(sample_responses == "S") / 4
print(paste('phat=',phat))
computed_phats[k]=phat
}
print(computed_phats)
# -----------------------------------------------------------------
### --- Simulating polls from population = 250 million American adults
pop_size = 250000000
p=0.88 # true population proportion
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
n=1000/4 # sample size
n_polls = 10000 # Number of polls
computed_phats = rep(0,n_polls)
for (k in 1:n_polls) {
sample_responses = sample(possible_responses, size = n)
phat = sum(sample_responses == "S") / n
computed_phats[k]=phat
}
hist(computed_phats,breaks=40)
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
# In Lab 3, you compare theoretical and empirical probabilities
# Recall: sum(dairyqueen$cal_fat > 600)/length(dairyqueen$cal_fat)
#
# Examples related to OpenIntro Chapter 5
# Sec5.1 Point Estimates and sampling variability
###---- Toy simulation: choosing small values to see what we are doing
pop_size = 20
p=0.8 # true population proportion
n=4 # sample size
# 1. Create poll responses
possible_responses = c(rep("S", 0.8*pop_size),rep("N", 0.2*pop_size))
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
print(possible_responses)
# 2. Sample 4 responses without replacement
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
#3. Proportion of responses with "Support" (S)
sum(sample_responses == "S") / 4
#Repeat the poll 5 times
computed_phats = rep(0,5)
for (k in 1:5) {
sample_responses = sample(possible_responses, size = 4)
print(sample_responses )
phat = sum(sample_responses == "S") / 4
print(paste('phat=',phat))
computed_phats[k]=phat
}
print(computed_phats)
# -----------------------------------------------------------------
### --- Simulating polls from population = 250 million American adults
pop_size = 250000000
p=0.88 # true population proportion
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
n=1000/4 # sample size
n_polls = 10000 # Number of polls
computed_phats = rep(0,n_polls)
for (k in 1:n_polls) {
sample_responses = sample(possible_responses, size = n)
phat = sum(sample_responses == "S") / n
computed_phats[k]=phat
}
hist(computed_phats,breaks=40)
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
# In Lab 3, you compare theoretical and empirical probabilities
# Recall: sum(dairyqueen$cal_fat > 600)/length(dairyqueen$cal_fat)
#
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
pop_size = 250000000
p=0.88 # true population proportion
possible_responses = c(rep("S", p*pop_size),rep("N", (1-p)*pop_size))
n=1000/4 # sample size
n_polls = 10000 # Number of polls
computed_phats = rep(0,n_polls)
for (k in 1:n_polls) {
sample_responses = sample(possible_responses, size = n)
phat = sum(sample_responses == "S") / n
computed_phats[k]=phat
}
hist(computed_phats,breaks=40)
print(paste('Mean = ', mean(computed_phats)))
print(paste('Standard Error = ', sd(computed_phats)))
# In Lab 3, you compare theoretical and empirical probabilities
# Recall: sum(dairyqueen$cal_fat > 600)/length(dairyqueen$cal_fat)
#
dpois(4, 5)
1 - dpois(4, 5)
ppois(4, 5)
1 - ppois(4, 5)
1 - ppois(3, 5)
1 - ppois(3, 5)
1 - pposoi(9, 20)
1 - ppoois(9, 20)
1 - ppois(9, 20)
ppois(9, 20)
dpois(9, 20)
1 - dpois(9, 20)
dpois(9, 20)
ppois(9, 20)
ppois(10, 20)
dgeom(1500, 0.002)
1- dgeom(1500, 0.002)
dbinom(1500, 0.002)
dbinom(8,1500, 0.002)
head(iris)
1/9 * 1/36
.41 + .37 _ .16
.41 + .37 + .16
(0 * .41) + (1 * .37) + (2 * .16) + (3 * 3.05) + (4 * .01)
(0 * .41) + (1 * .37) + (2 * .16) + (3 * .05) + (4 * .01)
.41 + .37
.78 _ .16
.78 + .16
(0 * .41) + (1 * .37) + (4 * .16) + (9 * .05) + (16 * .01)
.88 * .88
1.62 - .7744
sqrt(0.8456)
0.05 + 0.02 + 0.05 + 0.10 + 0.03 + 0.10
0.1 + 0.03 + 0.10 + 0.05 + 0.02 + 0.05
data(iris)
mean(iris$Petal.Width)
sd(iris$Petal.Width)
fivenum(Petal.Length)
fivenum(iris$Petal.Length)
five_num <- fivenum(iris$Petal.Length)
help(five_num)
help(fivenum)
quantile(iris$Petal.Length, 0.25)
1 - quantile(iris$Petal.Length, 0.60)
quantile(iris$Petal.Length, 0.60)
fivenum(iris.Petal.Length)
fivenum(iris$Petal.Length)
quantile(iris$Petal.Length, 0.60)
dpois(500, 0.09)
ppois(500, 0.09)
pgeom(500, 0.09)
pbinom(0.91, 500)
pbinom(0.91, 500, 0.09)
pbinom(500, 5000, 0.09)
stdev <- 4.78
mu <- 72.6
qnorm(80, mu, stdev)
pnorm(, mu, stdev)
1 -pnorm(80, mu, stdev)
1 -pnorm(80, 72.6, 4.78)
pnorm(80, 72.6, 4.78) - pnorm(60, 72.6, 4.78)
qnorm(0.05, 72.6, 4.78)
1 -qnorm(0.05, 72.6, 4.78)
qnorm(0.05, 72.6, 4.78)
qnorm(0.95, 72.6, 4.78)
pnbinom(7, 6, .55)
pnbinom(7, 6, .55)
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.975)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(lowerBound, upperBound)
c(phat-MOE,phat+MOE)
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.9)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(lowerBound, upperBound)
c(phat-MOE,phat+MOE)
zcrit = qnorm(0.99)
# march 28th
# plitical poll: find 95% confidence interval
# inputs
n <- 250
phat <- 0.58
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.99)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(lowerBound, upperBound)
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
zcrit = qnorm(0.975)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
zcrit = qnorm(0.975)
MOE = zcrit*SE
lowerBound = phat - MOE
upperBound = phat + MOE
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
zcrit = qnorm(0.975)
MOE = zcrit*SE
c(phat-MOE,phat+MOE)
n <- 1000
phat <- 0.887
SE = sqrt(phat*(1-phat)/n)
zcrit = qnorm(0.975)
MOE = zcrit*SE
c(phat-MOE,phat+MOE)
qnorm(0.01475)
# random sample of 1,042
1042 * .82
phat <- (1042*.82) / 1042
phat <- (1042*.82) / 1042
se <- sqrt((phat*(1-phat))/1042)
moe <- 1.96 * phat
lower <- phat - moe
upper <- phat + moe
# poll from 2014
# 82% of New Yorkers favor a manatory quarantine
# for anyone who came into contact with an EBola patient
# random sample of 1,042
phat <- (1042*.82) / 1042
se <- sqrt((phat*(1-phat))/1042)
moe <- 1.96 * phat
lower <- phat - moe
upper <- phat + moe
# poll from 2014
# 82% of New Yorkers favor a manatory quarantine
# for anyone who came into contact with an EBola patient
# random sample of 1,042
phat <- (1042*.82) / 1042
se <- sqrt((phat*(1-phat))/1042)
moe <- 1.96 * se
lower <- phat - moe
upper <- phat + moe
lower
upper
pnorm(0.58)
1 - qnorm(0.58)
# area to the right
1 - pnorm(2.53)
# area to the right
1 - pnorm(0.58, 0.5, 0.0316)
# area to the right
1 - pnorm(2.53)
# area to the right
1 - pnorm(0.58, 0.5, 0.0316)
# area to the right
1 - pnorm(2.53)
rm(income, insurance, region, uninsured)
# Lab 7, March 27th, 2025
library(dplyr)
library(ggplot2)
library(janitor)
library(readxl)
setwd("/Users/davidsosa/Code/school/regression-analysis/labs/lab7")
life_exp <- read_excel("data/life_expectancy_insurance_example_data.xlsx", sheet="Life Expectancy")
head(life_exp)
life_exp <- life_exp |>
filter(Year == 2010) |>
clean_names() |>
select(state, county, fips, female_le, male_le)
income <- read_excel("data/life_expectancy_insurance_example_data.xlsx", sheet="Income")
income$fips <- income$STATEA*1000 + income$COUNTYA
income <- income |>
clean_names() |>
select(fips, median_hh_income)
life_exp <- merge(life_exp, income, by = "fips")
# insurance part
insurance <- read_excel("data/life_expectancy_insurance_example_data.xlsx",sheet ="Insurance")
insurance$FIPS = 1000*insurance$STATEA + insurance$COUNTYA
uninsured <- insurance |>
clean_names() |>
group_by(fips) |>
summarize(uninsured_rate = (male_under65_uninsured + female_under65_uninsured)/(male_under65_total + female_under65_total))
life_exp <- merge(life_exp, uninsured, by = "fips")
region <- read_excel("data/life_expectancy_insurance_example_data.xlsx",sheet ="Region")
life_exp <- merge(life_exp, region, by.x = "state", by.y = "STATE")
rm(income, insurance, region, uninsured)
View(life_exp)
cor(life_exp$uninsured_rate, life_exp$female_le)
ggplot(data = life_exp) +
geom_point(aes(x = uninsured_rate, y = female_le)) +
xlab("Uninsured Rate") +
ylab("Female Life Expectancy") +
theme_classic()
summary(lm(female_le ~ uninsured_rate, data = life_exp))
summary(life_exp$uninsured_rate)
summary(lm(female_le ~ uninsured_rate + median_hh_income, data = life_exp))
summary(lm(female_le ~ uninsured_rate + median_hh_income + Region, data = life_exp))
summary(lm(female_le ~ uninsured_rate, data = life_exp))
summary(life_exp$uninsured_rate)
summary(lm(female_le ~ uninsured_rate + median_hh_income, data = life_exp))
summary(lm(female_le ~ uninsured_rate + median_hh_income + Region, data = life_exp))
spending <- read_excel("homelessness_data.xlsx", "Federal Aid")
spending <- read_excel("data/homelessness_data.xlsx", "Federal Aid")
state_spending <- spending |>
group_by(state_code) |>
summarize(funding = sum(amount))
spending$state_code <- substr(spending$coc_number, 1, 2)
state_spending <- spending |>
group_by(state_code) |>
summarize(funding = sum(amount))
